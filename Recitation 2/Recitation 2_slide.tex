\documentclass[aspectratio=169]{beamer}

\mode<presentation>
\usetheme{Boadilla}
\definecolor{Khaki}{RGB}{144,110,62}
\definecolor{blue}{RGB}{30,90,205}
\definecolor{red}{RGB}{213,94,0}
\definecolor{green}{RGB}{0,128,0}
\setbeamercolor{title}{fg=Khaki}
\setbeamercolor{frametitle}{fg=Khaki}
\setbeamercolor{block title}{bg=Khaki, fg=white}
\setbeamercolor{block body}{bg=white}
\setbeamercolor{structure}{fg=Khaki}
\setbeamercolor{item projected}{fg=white}
\setbeamercolor{item}{fg=Khaki}
\setbeamercolor{subitem}{fg=Khaki}
\setbeamercolor{section in toc}{fg=Khaki}
\setbeamercolor{description item}{fg=Khaki}
\setbeamercolor{caption name}{fg=Khaki}
\setbeamercolor{button}{bg=Khaki, fg=white}
\setbeamercolor{caption name}{fg=Khaki}
\usepackage{graphics}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{bbm}
\usetikzlibrary{decorations.pathreplacing}
\usepackage{geometry}
\usepackage{booktabs}
\usepackage{bookmark}
\usepackage{multirow, makecell}
\usepackage{float}
\usepackage{fancyvrb}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{hyperref}
\usepackage{threeparttable}
\usepackage{appendixnumberbeamer} 
\usepackage[T1]{fontenc}
\usepackage[default]{lato} 

\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}
\newenvironment{wideenumerate}{\enumerate\addtolength{\itemsep}{10pt}}{\endenumerate}
\newenvironment{widedescription}{\description\addtolength{\itemsep}{10pt}}{\enddescription}
\hypersetup{
colorlinks=true,
linkcolor=black,
filecolor=green, 
urlcolor=blue,
}
\beamertemplatenavigationsymbolsempty
\setbeamercolor{author in head/foot}{bg=white, fg=Khaki}
\setbeamercolor{title in head/foot}{bg=white, fg=Khaki}
\setbeamercolor{date in head/foot}{bg=white, fg=Khaki}
\setbeamercolor{section in head/foot}{bg=white, fg=Khaki}
\setbeamercolor{page number in head/foot}{bg=white, fg=Khaki}
\setbeamercolor{headline}{bg=white}
\setbeamertemplate{footline}{
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.33\paperwidth,ht=2.25ex,dp=1ex,center]{date in head/foot}%
            \usebeamerfont{date in head/foot}\insertshortdate
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.44\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.22\paperwidth,ht=2.25ex,dp=1ex,center]{page number in head/foot}%
            \usebeamerfont{page number in head/foot}\insertframenumber{} / \inserttotalframenumber
        \end{beamercolorbox}}%
        \vskip0pt%
    }
%\setbeamercolor{page number in head/foot}{fg=black}
\setbeamertemplate{section in toc}[sections numbered]
\setbeamertemplate{subsection in toc}{\leavevmode\leftskip=3em\rlap{\hskip-1.75em\inserttocsectionnumber.\inserttocsubsectionnumber}\inserttocsubsection\par}
\setbeamerfont{subsection in toc}{size=\footnotesize}

\newenvironment{transitionframe}{\setbeamercolor{background canvas}{bg=Khaki}\setbeamertemplate{footline}{} \begin{frame}}{\end{frame}}


\makeatletter
\let\@@magyar@captionfix\relax
\makeatother


\title[Recitation 2 (UN 3412)]{Recitation 2: Ordinary least squares} % Change this regularly
\author[Lee]{Seung-hun Lee}
\institute[]{Columbia University \\ Undergraduate Introduction to Econometrics Recitation}

\date[September 22nd, 2022]{September 22nd, 2022}

\begin{document}
\begin{frame}
\titlepage
\end{frame}


%%% Color slides for section headers: Use for colloquium version (The ones bewteen \iffals and \fi)



\begin{transitionframe}
  \begin{center}
         { \Huge \textcolor{white}{Econometrics and RCT}}
       \end{center}
\end{transitionframe}

\begin{frame}
\frametitle{What is econometrics trying to achieve?}
\begin{wideitemize}
\item Econometrics is a field in economics that tries to answer real life questions.  
\begin{itemize}
\item Ultimately, it is about making a \textbf{quantitative} statement about two or more random events
\item We can make \textbf{correlational} statements, but we want to identify \textit{causal relationships}
\end{itemize}
\item In order to achieve this goal, we collect data from a suitably defined population and use various methods to estimate a parameter that implies correlational/causal relationship.
\item To fully understand what econometrics is trying to achieve, we need to ask ourselves these three questions
\begin{itemize}
\item  What is the difference between correlational and causal relation? 
\item What is the suitably defined population? 
\item What are the methods that we need to use in econometrics?
\end{itemize}
 \end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Correlation vs Causation}
\begin{wideitemize}
\item Suppose you have two random variables $X$ and $Y$. You want to identify if $X$ \textit{causes} $Y$
\item A \textbf{correlation} between $X$ and $Y$ is a statistical measure that describes how the two variables move together
\begin{itemize}
\item It captures \textit{any} type of statistical dependence that moves the two variables together: Causation, but also others too!
\item Not causal 1: $Y$ can cause $X$
\item Not causal 2: $X$ and $Y$ are jointly moving because there is $Z$ that affects both
\end{itemize}
\item A \textbf{causal} relationship: \textit{Cleanly (exogenously)} changing variables $X$ leads to changes in $Y$
\begin{itemize}
\item Much more difficult: Changes in $X$ may be a combination of many things
\item Changes from $X$ alone and changes from other factors that may indirectly affect $X$
\item RCT: Isolates clean changes in $X$ that can help us tell whether changes in $X$ affects $Y$, and by how much
\item Econometrics: We can express concisely the relationship between $X$ and $Y$ variables in a single equation. 
\end{itemize}
 \end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Suitably defined population?}
\begin{wideitemize}
\item When we say we are interested in the relationship between schooling and wages, whose effects are we interested in? 
\begin{itemize}
\item The entire US population, high school graduates, or college graduates?  
\item Determines sampling methods we use to obtain a representative and comparable sample
\item Complete randomization, stratified randomization, or cluster randomization
\end{itemize}
\item  Note: We are almost surely never going to get the data from the entire population. 
\begin{itemize}
\item Gathering data from the entire population is logistically (and maybe ethically) difficult. 
\item The estimate we are obtaining through any econometric exercise is thus a \textit{sample analogue} of the actual value we are trying to get
\item We will do diagnostic tests to see if they can be reasonably close to the true value. 
\end{itemize}
 \end{wideitemize}
\end{frame}


\begin{frame}
\frametitle{What methods?}
\begin{wideitemize}
\item In econometrics, we will use many estimation methods to obtain the sample analogue of the parameter of interest. 
\begin{itemize}
\item Ordinary least squares (OLS): Suitable for randomized control trials or in any case where the treatment assignment is as good as random.
\item Panel estimation: If data has multiple individuals and multiple time periods.
\item Instrumental variable methods: When we have proxy variables relevant to variable of interest and is reasonably exogenous
\item Difference-in-differences: When we study `before \& after' events with multiple entities
\item Regression discontinuity: In treatment with a cutoff determining treatment assignment
\item Time series: When we observe one entity over multiple periods 
\end{itemize}
\item Depending on the type of variables we use in our exercise, we have:
\begin{itemize}
\item Univariate regression: One variable (besides an overall constant) controlled for
\item Multivariate regression: Multiple variables (besides an overall constant) controlled for
\item Nonlinear regression: Binary dependent variables
\item Big data methods
\end{itemize}
 \end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Understanding RCTs}
\begin{wideitemize}
\item In \textbf{randomized control trials}, we randomly categorize some individuals under treatment group and controlled group and run various tests
\begin{itemize}
\item Benchmark for good program evaluation
\end{itemize}
\item Potential outcomes framework
\begin{itemize}
\item $Y_i$ : Observed outcome for individual $i\in\{1,...,N\}$
\item $i$: Either in treatment or control group (not both)$\to$ $W_i=1$ if $i$ is treated, 0 if otherwise
\item $\mathbf{W}$:  an $N$-tuple vector of treatment assignment for all individuals
\item Key assumption: Others' treatment assignment has no effect on my treatment (stable unit treatment value assumption (SUTVA))
\item Potential outcome $Y_i(w)$: Outcome for treated ($Y_i(1)$) and the untreated individual ($Y_i(0)$)
\begin{itemize}
\item Fundamental problem of missing data: Individual $i$ cannot have both $Y_i(1)$ and $Y_i(0)$ - at most one of them
\end{itemize}
\end{itemize}
 \end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Potential vs observed outcome}
\begin{wideitemize}
\item We can bridge the two with this relation
\[
\begin{aligned}
Y_ i &= Y_i(1)W_i + Y_i(0)(1-W_i)\\
&=Y_i(0)+W_i(Y_i(1)-Y_i(0))\\
\end{aligned}
\]
\begin{itemize}
\item We know $W_i$ and $Y_i$ for everyone regardless of treatment assignment
\item We cannot see $Y_i(0)$ for the treated group and $Y_i(1)$ for the untreated group
\end{itemize}
 \end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Treatment effect}
\begin{wideitemize}
\item If we want to see if the treatment has any effect, we would ideally see
\[
Y_i(1)-Y_i(0)
\]
\item But they cannot be obtained b/c fundamental problem of missing data
\begin{itemize}
\item Alternative is average treatment effect or average treatment effect on the treated
\[
\begin{aligned}
\text{ATE}&=E[Y_i(1)-Y_i(0)]\\
\text{ATT}&=E[Y_i(1)-Y_i(0)|W_i=1]
\end{aligned}
\]
\item We also need assumptions about our treatment: Randomized assignment is one of them
\[
\begin{aligned}
E[Y_i(1)]&= E[Y_i(1)|W_i=1]=E[Y_i(1)|W_i=0] \\
E[Y_i(0)]&= E[Y_i(0)|W_i=1]=E[Y_i(0)|W_i=0] 
\end{aligned}
\]
\end{itemize}
 \end{wideitemize}
\end{frame}

\begin{frame}
\frametitle{Obtaining ATE}
\begin{wideitemize}
\item With this assumption and the definition of potential outcomes framework
\[
\begin{aligned}
E[Y_i|W_i]&=E[Y_i(1)W_i + Y_i(0)(1-W_i)|W_i]\\
&=E[Y_i(1)|W_i]W_i+E[Y_i(0)|W_i](1-W_i)\\
&=E[Y_i(1)]W_i+E[Y_i(0)](1-W_i)\\
\end{aligned}
\]
\begin{itemize}
\item $W_i=1$: Get $E[Y_i(1)]=E[Y_i|W_i=1]$.
\item $W_i=0$: Get $E[Y_i(0)]=E[Y_i|W_i=0]$. 
\end{itemize}
\item Under random assignment, we can identify the average treatment effect as
\[
\text{ATE}=E[Y_i(1)]-E[Y_i(0)] = E[Y_i|W_i=1]-E[Y_i|W_i=0]
\]
\item Econometrically: OLS with $W_i$ as independent variable (Problem set 2!)
 \end{wideitemize}
\end{frame}

\begin{transitionframe}
  \begin{center}
         { \Huge \textcolor{white}{Ordinary least squares}}
       \end{center}
\end{transitionframe}

\begin{frame}
\frametitle{Ordinary Least Squares: Population vs sample linear models}
\begin{itemize}
\item Suppose that the \textbf{population linear regression model} (also known as data generating process in some books) is
\[
Y_i = \beta_0 + \beta_1X_i + u_i
\]
\item However, we do not know the true values of the population parameters - $\beta_0$ and $\beta_1$
\item An alternative way to approach the problem is to use the \textbf{sample linear regression model} (or just model)
\[
Y_i = \hat{\beta}_0 +\hat{\beta}_1X_i +\hat{u}_i
\]
where $\hat{\beta}_0, \hat{\beta}_1$ are estimates of ${\beta}_0, {\beta}_1$

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ordinary Least Squares: Definition}
\begin{itemize}
\item The ideal estimator minimizes the squared sum of residuals. 
\item Mathematically, this can be obtained by solving the following minimization problem and the first order conditions
\footnotesize{\begin{gather*}
\min_{\hat{\beta}_0, \hat{\beta}_1} \sum_{i=1}^n (Y_i-\hat{\beta}_0 - \hat{\beta}_1X_i)^2\\
[\hat{\beta}_0]: -2\sum_{i=1}^n(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0\\
[\hat{\beta}_1]: -2\sum_{i=1}^nX_i(Y_i-\hat{\beta}_0-\hat{\beta}_1X_i)=0 
\end{gather*}}\normalsize
The resulting \textbf{least squares estimators} are
\[
\hat{\beta}_0 = \bar{Y}-\hat{\beta}_1\bar{X}, \ \ \hat{\beta}_1=\frac{\sum_{i=1}^n(Y_i-\bar{Y})(X_i-\bar{X})}{\sum_{i=1}^n(X_i-\bar{X})^2}
\]
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Ordinary Least Squares: Main assumptions}
\begin{itemize}
\item For OLS to be unbiased, consistent, efficient, and asymptotic normal, the following assumptions must be made
\begin{block}{Assumptions}
\begin{itemize}
\item[\textbf{A0}] Linearity: The regression is assumed to be linear in parameters.
\[
\text{Okay: } Y_i = \beta_0 + \beta_1X_i + \beta_2X_i^2+u_i, \ \text{Not: } Y_i = \beta_0 + \beta_1X_i + \beta_2^2X_i+u_i
\]
\item[\textbf{A1}] $E(u_i|X_i)=0$: Conditional on letting $X_i$ take a certain value, we are not making any systematical error in the linear regression. This is required for the OLS to be unbiased. (or $cov(X_i, u_i)=0$)
\item[\textbf{A2}] i.i.d. (random sampling): $(X_i,Y_i)$ is assumed to be from independent, identical distribution
\item[\textbf{A3}] No Outliers: Outlier has no impact on the regression results. ($E(X_i^4), E(Y_i^4)<\infty$)
\item[\textbf{A4}] Homoskedasticity: $var(u_i)=\sigma_u$ (variance of $u_i$ does not depend on $X_i$). $\leftrightarrow$ \textit{heteroskedasticity}
\item[\textbf{A5}] No Autocorrelation (Serial Correlation): For $i\neq j$, $cov(u_i,u_j)=0$. Error at the previous period does not have any impact on the current period. This is usually broken in time series settings

\end{itemize}
\end{block}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Ordinary Least Squares: Useful alternative expression for $\hat{\beta}_1$ }
\begin{itemize}
\item OLS estimate that we are getting is a random variable - getting different estimates depending on sample we work with. 
\item$\hat{\beta}_1$: Recall that we can write
\[
\hat{\beta}_1= \frac{\sum_{i=1}^n(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^n(X_i-\bar{X})^2}
\]
Now, replace $Y_i$ an $\bar{Y}$ with 
\[
Y_i =\beta_0 + \beta X_i + u_i, \ \bar{Y} = \beta_0 + \beta_1\bar{X} + \bar{u},
\]
which allows us to write 
\[
(Y_i-\bar{Y}) = (\beta_1(X_i-\bar{X})+(u_i-\bar{u}))
\]
and get
\[
\hat{\beta}_1=\beta_1+  \frac{\sum_{i=1}^n(X_i-\bar{X})(u_i-\bar{u})}{\sum_{i=1}^n(X_i-\bar{X})^2}
\]

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Ordinary Least Squares: Unbiasedness of $\hat{\beta}_1$}
\begin{itemize}
\item $E[\hat{\beta}_1]$: It can be written as
\small{\[
\begin{aligned}
E[\hat{\beta}_1]& = E\left[\beta_1+  \frac{\sum_{i=1}^n(X_i-\bar{X})(u_i-\bar{u})}{\sum_{i=1}^n(X_i-\bar{X})^2}\right]\\
&=\beta_1+ E\left[\frac{\sum_{i=1}^n(X_i-\bar{X})(u_i-\bar{u})}{\sum_{i=1}^n(X_i-\bar{X})^2}\right]
\end{aligned}
\]}\normalsize
$\sum_{i=1}^n(X_i-\bar{X})(u_i-\bar{u})$ can be written to something simpler.
\small{\[
\sum_{i=1}^n(X_i-\bar{X})(u_i-\bar{u})=\sum_{i=1}^nX_iu_i-\bar{u}\sum_{i=1}^n X_i-\bar{X}\sum_{i=1}^nu_i+n\bar{X}\bar{u} = \sum_{i=1}^n(X_i-\bar{X})u_i
\]}\normalsize
\item[$\to$] Since $\bar{X}$ is a sample mean of $X$, $\sum_{i=1}^nX_i=n\bar{X}$. \\
\item[$\to$] The assumption that conditional mean is zero and $(X_i, u_i)$ are uncorrelated means that the term on the left hand side is zero. 
\item[$\to$] Therefore, UNDER CLASSICAL ASSUMPTIONS, $E[\hat{\beta}_1]=\beta_1$.

\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Ordinary Least Squares: Unbiasedness of $\hat{\beta}_0$}
\begin{itemize}
\item $\hat{\beta}_0$: The formula for $\hat{\beta}_0$ is $\bar{Y}-\hat{\beta}_1\bar{X}$. By changing $\bar{Y}$, we can get
\[
\begin{aligned}
\hat{\beta}_0&=(\beta_0+\beta_1\bar{X}+\bar{u})-\hat{\beta}_1\bar{X}\\
&=\beta_0+(\beta_1-\hat{\beta}_1)\bar{X}+\bar{u}
\end{aligned}
\]
Then we can say the following about the sampling distribution
\item $E[\hat{\beta}_0]$: We can write
\[
E[\hat{\beta}_0]=\beta_0+E[(\beta_1-\hat{\beta}_1)\bar{X}]+E[\bar{u}]=\beta_0
\]
since $\hat{\beta}_1$ is unbiased and conditional expectation of $u_i$ is zero. 
\item[$\to$]Thus, under our current assumptions, $\hat{\beta}_0$ is unbiased. 
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ordinary Least Squares: Variances of $\hat{\beta}_0$ and $\hat{\beta}_1$}
\begin{itemize}
\item Might take bit of a work, but when you follow the notes, you get
\[
var(\hat{\beta}_0)= \frac{\sigma_u^2}{n}\frac{\sum_{i=1}^nX_i^2}{\sum_{i=1}^n(X_i-\bar{X})^2}, var(\hat{\beta}_1)=\frac{\sigma_u^2}{\sum_{i=1}^n(X_i-\bar{X})^2}
\]
\item At the end of the day, we can say
\begin{gather*}
 \hat{\beta}_1 \sim N\left(\beta_1, \frac{\sigma_u^2}{\sum_{i=1}^n(X_i-\bar{X})^2}\right) \\
\hat{\beta}_0 \sim N\left(\beta_0, \frac{\sigma_u^2}{n}\frac{\sum_{i=1}^nX_i^2}{\sum_{i=1}^n(X_i-\bar{X})^2}\right)
\end{gather*}
\item The importance of this is that now we can conduct a hypothesis test and create a test statistic based on this distribution
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ordinary Least Squares: How well does the model capture the data?}
Measure of fitness
\begin{itemize}
\item These numbers tell us how informative the sample linear regression we used is in telling us about the population data
\item $\mathbf{R^2}$: It is defined as a fraction of total variation which is explained by the model. Mathematically, this is
\footnotesize{%
\begin{gather*} 
Y_i = \underbrace{\hat{\beta}_0 + \hat{\beta}_1X_i}_{\hat{Y}_i} + u_i, \ \bar{Y} = \underbrace{\hat{\beta}_0 + \hat{\beta}_1\bar{X}}_{\bar{\hat{Y}}} + \bar{u}, \\
\implies Y_i-\bar{Y} = (\hat{Y}_i - \bar{\hat{Y}}) - (u_i - \bar{u}) \\
\implies \sum_{i=1}^n (Y_i-\bar{Y})^2= \sum_{i=1}^n (\hat{Y}_i - \bar{\hat{Y}})^2+\sum_{i=1}^n(u_i - \bar{u})^2 - 2\sum_{i=1}^n(\hat{Y}_i - \bar{\hat{Y}}) (u_i - \bar{u})
\end{gather*}}\normalsize
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Ordinary Least Squares: Getting to $R^2$}
\begin{itemize}
\item Note that 
\footnotesize{\[
\sum_{i=1}^n(\hat{Y}_i - \bar{\hat{Y}}) (u_i - \bar{u})=\sum_{i=1}^n\hat{Y}_i{u}_i-\bar{\hat{Y}}\sum_{i=1}^nu_i -\bar{u}\sum_{i=1}^n\hat{Y}_i +n\bar{u}\bar{\hat{Y}}
\]}\normalsize
\item Since $\sum_{i=1}^nu_i = n\bar{u},\ \sum_{i=1}^n\hat{Y}_i = n\bar{\hat{Y}}$ and $\sum_{i=1}^n\hat{Y}_iu_i=n\bar{u}\bar{\hat{Y}}$, all terms cancel each other out. 
\item So we are left with
\footnotesize{\begin{gather*}
\underbrace{\sum_{i=1}^n (Y_i-\bar{Y})^2}_{TSS}= \underbrace{\sum_{i=1}^n (\hat{Y}_i - \bar{\hat{Y}})^2}_{ESS}+\underbrace{\sum_{i=1}^n(u_i - \bar{u})^2}_{RSS} \\
\implies 1=\frac{\sum_{i=1}^n (\hat{Y}_i - \bar{\hat{Y}})^2}{\sum_{i=1}^n (Y_i-\bar{Y})^2} + \frac{\sum_{i=1}^n(u_i - \bar{u})^2 }{\sum_{i=1}^n (Y_i-\bar{Y})^2}
\end{gather*}}\normalsize
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Ordinary Least Squares: Getting to $R^2$}
\begin{itemize}
\item Thus, the $R^2$ can be found as
\[
R^2 = \frac{\sum_{i=1}^n (\hat{Y}_i - \bar{\hat{Y}})^2}{\sum_{i=1}^n (Y_i-\bar{Y})^2} = \frac{ESS}{TSS} = 1-\frac{RSS}{TSS}
\]
\item Intuitively, higher $R^2$ implies that the model explains more of the total variance, which implies that the regression fits the data well. 
\end{itemize}
\end{frame}

%%%%%%%%%%%
\end{document}
